{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de PCA en NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "* Implementación de PCA en NumPy paso a paso\n",
    "* Comparación de resultados con Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dado un dataset $X \\in \\mathbb{R}^{n, d}$, con $n$ muestras y $d$ features, queremos reducir sus dimensiones a $m$. Para ello, el primer paso es centrar el dataset (Hint: usen np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0.8,0.7],[0.1,-0.1]])\n",
    "    \n",
    "X_norm= X - np.mean(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtener la matriz de covarianza de $X^T$, revisar en la teoría por qué utilizamos la transpuesta. Buscar en la documentación de NumPy qué funciones se pueden utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "S= np.cov(X_norm.transpose())/(X_norm.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calcular los autovalores y autovectores de la matriz de covarianza. Revisar la documentación de NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,v=np.linalg.eig(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ordernar los autovectores en el sentido de los autovalores decrecientes, revisar la teoría de ser necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.65850461 -0.75257669]\n",
      " [-0.75257669  0.65850461]]\n"
     ]
    }
   ],
   "source": [
    "eigenvalues_order=np.argsort(w)\n",
    "eigenvalues_order=eigenvalues_order[::-1]\n",
    "w=w[eigenvalues_order]\n",
    "v=v[:,eigenvalues_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Proyectar el dataset centrado sobre los $m$ autovectores más relevantes (Hint: usen np.dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=1\n",
    "b=v[:,0:m]\n",
    "X_pca= np.matmul(b.transpose(),X_norm.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Consolidar los pasos anteriores en una función o clase PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca(X, n_components):\n",
    "    \"\"\"\n",
    "    Transformacion de vectores X segun PCA\n",
    "    \n",
    "    Args:\n",
    "        X (numpy array): vectores\n",
    "        n_components (int): cantidad de componentes a considerar\n",
    "    Returns:\n",
    "        numpy array: vecotres transformados\n",
    "    \"\"\"\n",
    "    X_norm= X - np.mean(X,axis=0)\n",
    "    S= np.cov(X_norm.transpose())/(X_norm.shape[0])\n",
    "    w,v=np.linalg.eig(S)\n",
    "    eigenvalues_order=np.argsort(w)\n",
    "    eigenvalues_order=eigenvalues_order[::-1]\n",
    "    w=w[eigenvalues_order]\n",
    "    v=v[:,eigenvalues_order]\n",
    "\n",
    "    b=v[:,0:n_components]\n",
    "    X_pca= np.dot(b.transpose(),X_norm.transpose())\n",
    "    \n",
    "    return X_pca.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Comparar los resultados obtenidos con el modelo de PCA implementado en Scikit-learn ([ver documentación](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)). Tomar como dataset:\n",
    "\n",
    "$X=\\begin{bmatrix}\n",
    "0.8 & 0.7\\\\\n",
    "0.1 & -0.1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Se debe reducir a un componente. Verificar los resultados con np.testing.assert_allclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0.8,0.7],[0.1,-0.1]])\n",
    "X_pca_1=my_pca(X,1)\n",
    "X_norm= X - np.mean(X,axis=0)\n",
    "pca = PCA(n_components=1)\n",
    "X_pca_2 = pca.fit_transform(X_norm)\n",
    "np.testing.assert_allclose(X_pca_1, X_pca_2, rtol=1e-7)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
